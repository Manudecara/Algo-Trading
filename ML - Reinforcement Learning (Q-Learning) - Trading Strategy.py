{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from model import mlp\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent(object):\n",
    "  \"\"\" A simple Deep Q agent \"\"\"\n",
    "  def __init__(self, state_size, action_size):\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.memory = deque(maxlen=2000)\n",
    "    self.gamma = 0.95  # discount rate\n",
    "    self.epsilon = 1.0  # exploration rate\n",
    "    self.epsilon_min = 0.01\n",
    "    self.epsilon_decay = 0.995\n",
    "    self.model = mlp(state_size, action_size)\n",
    "\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return random.randrange(self.action_size)\n",
    "    act_values = self.model.predict(state)\n",
    "    return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "\n",
    "  def replay(self, batch_size=32):\n",
    "    \"\"\" vectorized implementation; 30x speed up compared with for loop \"\"\"\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "    states = np.array([tup[0][0] for tup in minibatch])\n",
    "    actions = np.array([tup[1] for tup in minibatch])\n",
    "    rewards = np.array([tup[2] for tup in minibatch])\n",
    "    next_states = np.array([tup[3][0] for tup in minibatch])\n",
    "    done = np.array([tup[4] for tup in minibatch])\n",
    "\n",
    "    # Q(s', a)\n",
    "    target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "    # end state target is reward itself (no lookahead)\n",
    "    target[done] = rewards[done]\n",
    "\n",
    "    # Q(s, a)\n",
    "    target_f = self.model.predict(states)\n",
    "    # make the agent to approximately map the current state to future discounted reward\n",
    "    target_f[range(batch_size), actions] = target\n",
    "\n",
    "    self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "      self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "  def load(self, name):\n",
    "    self.model.load_weights(name)\n",
    "\n",
    "\n",
    "  def save(self, name):\n",
    "    self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  A 3-stock (MSFT, IBM, QCOM) trading environment.\n",
    "  State: [# of stock owned, current stock prices, cash in hand]\n",
    "    - array of length n_stock * 2 + 1\n",
    "    - price is discretized (to integer) to reduce state space\n",
    "    - use close price for each stock\n",
    "    - cash in hand is evaluated at each step based on action performed\n",
    "  Action: sell (0), hold (1), and buy (2)\n",
    "    - when selling, sell all the shares\n",
    "    - when buying, buy as many as cash in hand allows\n",
    "    - if buying multiple stock, equally distribute cash in hand and then utilize the balance\n",
    "  \"\"\"\n",
    "  def __init__(self, train_data, init_invest=20000):\n",
    "    # data\n",
    "    self.stock_price_history = np.around(train_data) # round up to integer to reduce state space\n",
    "    self.n_stock, self.n_step = self.stock_price_history.shape\n",
    "\n",
    "    # instance attributes\n",
    "    self.init_invest = init_invest\n",
    "    self.cur_step = None\n",
    "    self.stock_owned = None\n",
    "    self.stock_price = None\n",
    "    self.cash_in_hand = None\n",
    "\n",
    "    # action space\n",
    "    self.action_space = spaces.Discrete(3**self.n_stock)\n",
    "\n",
    "    # observation space: give estimates in order to sample and build scaler\n",
    "    stock_max_price = self.stock_price_history.max(axis=1)\n",
    "    stock_range = [[0, init_invest * 2 // mx] for mx in stock_max_price]\n",
    "    price_range = [[0, mx] for mx in stock_max_price]\n",
    "    cash_in_hand_range = [[0, init_invest * 2]]\n",
    "    self.observation_space = spaces.MultiDiscrete(stock_range + price_range + cash_in_hand_range)\n",
    "\n",
    "    # seed and start\n",
    "    self._seed()\n",
    "    self._reset()\n",
    "\n",
    "\n",
    "  def _seed(self, seed=None):\n",
    "    self.np_random, seed = seeding.np_random(seed)\n",
    "    return [seed]\n",
    "\n",
    "\n",
    "  def _reset(self):\n",
    "    self.cur_step = 0\n",
    "    self.stock_owned = [0] * self.n_stock\n",
    "    self.stock_price = self.stock_price_history[:, self.cur_step]\n",
    "    self.cash_in_hand = self.init_invest\n",
    "    return self._get_obs()\n",
    "\n",
    "\n",
    "  def _step(self, action):\n",
    "    assert self.action_space.contains(action)\n",
    "    prev_val = self._get_val()\n",
    "    self.cur_step += 1\n",
    "    self.stock_price = self.stock_price_history[:, self.cur_step] # update price\n",
    "    self._trade(action)\n",
    "    cur_val = self._get_val()\n",
    "    reward = cur_val - prev_val\n",
    "    done = self.cur_step == self.n_step - 1\n",
    "    info = {'cur_val': cur_val}\n",
    "    return self._get_obs(), reward, done, info\n",
    "\n",
    "\n",
    "  def _get_obs(self):\n",
    "    obs = []\n",
    "    obs.extend(self.stock_owned)\n",
    "    obs.extend(list(self.stock_price))\n",
    "    obs.append(self.cash_in_hand)\n",
    "    return obs\n",
    "\n",
    "\n",
    "  def _get_val(self):\n",
    "    return np.sum(self.stock_owned * self.stock_price) + self.cash_in_hand\n",
    "\n",
    "\n",
    "  def _trade(self, action):\n",
    "    # all combo to sell(0), hold(1), or buy(2) stocks\n",
    "    action_combo = map(list, itertools.product([0, 1, 2], repeat=self.n_stock))\n",
    "    action_vec = action_combo[action]\n",
    "\n",
    "    # one pass to get sell/buy index\n",
    "    sell_index = []\n",
    "    buy_index = []\n",
    "    for i, a in enumerate(action_vec):\n",
    "      if a == 0:\n",
    "        sell_index.append(i)\n",
    "      elif a == 2:\n",
    "        buy_index.append(i)\n",
    "\n",
    "    # two passes: sell first, then buy; might be naive in real-world settings\n",
    "    if sell_index:\n",
    "      for i in sell_index:\n",
    "        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
    "        self.stock_owned[i] = 0\n",
    "    if buy_index:\n",
    "      can_buy = True\n",
    "      while can_buy:\n",
    "        for i in buy_index:\n",
    "          if self.cash_in_hand > self.stock_price[i]:\n",
    "            self.stock_owned[i] += 1 # buy one share\n",
    "            self.cash_in_hand -= self.stock_price[i]\n",
    "          else:\n",
    "            can_buy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "def mlp(n_obs, n_action, n_hidden_layer=1, n_neuron_per_layer=32,\n",
    "        activation='relu', loss='mse'):\n",
    "  \"\"\" A multi-layer perceptron \"\"\"\n",
    "  model = Sequential()\n",
    "  model.add(Dense(n_neuron_per_layer, input_dim=n_obs, activation=activation))\n",
    "  for _ in range(n_hidden_layer):\n",
    "    model.add(Dense(n_neuron_per_layer, activation=activation))\n",
    "  model.add(Dense(n_action, activation='linear'))\n",
    "  model.compile(loss=loss, optimizer=Adam())\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "from envs import TradingEnv\n",
    "from agent import DQNAgent\n",
    "from utils import get_data, get_scaler, maybe_make_dir\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('-e', '--episode', type=int, default=2000,\n",
    "                      help='number of episode to run')\n",
    "  parser.add_argument('-b', '--batch_size', type=int, default=32,\n",
    "                      help='batch size for experience replay')\n",
    "  parser.add_argument('-i', '--initial_invest', type=int, default=20000,\n",
    "                      help='initial investment amount')\n",
    "  parser.add_argument('-m', '--mode', type=str, required=True,\n",
    "                      help='either \"train\" or \"test\"')\n",
    "  parser.add_argument('-w', '--weights', type=str, help='a trained model weights')\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  maybe_make_dir('weights')\n",
    "  maybe_make_dir('portfolio_val')\n",
    "\n",
    "  timestamp = time.strftime('%Y%m%d%H%M')\n",
    "\n",
    "  data = np.around(get_data())\n",
    "  train_data = data[:, :3526]\n",
    "  test_data = data[:, 3526:]\n",
    "\n",
    "  env = TradingEnv(train_data, args.initial_invest)\n",
    "  state_size = env.observation_space.shape\n",
    "  action_size = env.action_space.n\n",
    "  agent = DQNAgent(state_size, action_size)\n",
    "  scaler = get_scaler(env)\n",
    "\n",
    "  portfolio_value = []\n",
    "\n",
    "  if args.mode == 'test':\n",
    "    # remake the env with test data\n",
    "    env = TradingEnv(test_data, args.initial_invest)\n",
    "    # load trained weights\n",
    "    agent.load(args.weights)\n",
    "    # when test, the timestamp is same as time when weights was trained\n",
    "    timestamp = re.findall(r'\\d{12}', args.weights)[0]\n",
    "\n",
    "  for e in range(args.episode):\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    for time in range(env.n_step):\n",
    "      action = agent.act(state)\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      next_state = scaler.transform([next_state])\n",
    "      if args.mode == 'train':\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "      state = next_state\n",
    "      if done:\n",
    "        print(\"episode: {}/{}, episode end value: {}\".format(\n",
    "          e + 1, args.episode, info['cur_val']))\n",
    "        portfolio_value.append(info['cur_val']) # append episode end portfolio value\n",
    "        break\n",
    "      if args.mode == 'train' and len(agent.memory) > args.batch_size:\n",
    "        agent.replay(args.batch_size)\n",
    "    if args.mode == 'train' and (e + 1) % 10 == 0:  # checkpoint weights\n",
    "      agent.save('weights/{}-dqn.h5'.format(timestamp))\n",
    "\n",
    "  # save portfolio value history to disk\n",
    "  with open('portfolio_val/{}-{}.p'.format(timestamp, args.mode), 'wb') as fp:\n",
    "    pickle.dump(portfolio_value, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
